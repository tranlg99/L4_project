## Notes ##
### Work done
- generated full dataset (4571 samples) - first part was corrupted for some reason??
- looked into reconstruing frames with interpolation - check logic, interpolate coords or rgb values?
- max batch size investigation - B=14

Diss background, analysis
   - long range motion flow: PIPs
   - future frame prediction: single input, sequence input - overview, what are the limitations
   - CNN, why are we using this model
    
Motivation
   - directly outputing RGB pixel values commonly results in blurry images, instead predict last position of trajectory of pixels
   - -> this trajectory-supervised modeling requires labeling (considerable human time and effort), which can be avoided using PIPs

__Dataset__

First part of dataset ("-CR4xjdQbkc", "MGYF1aDwUKg", "vB8XTJfV4rY", "KjMxdYJOwqI", "asL3ZyuNeB0", "KMO1BluPtU4", "sm6-dbG5Rho") was corrupted, although the motion flow frames from the same videos were already correctly generated by PIPs before.
I will generate and check them again.

__Reconstruction of frames__
Logic for reconstruction was correct, but many NaN were produced. Paul advised to switch to NearestNDInterpolator rather than using LinearNDInterpolator.

I have tried on on ground truth coordinates and that worked. However, I have also included invisible points.

## Plan ##
* scale up - train on full dataset
* re-tune lr
* reconstruct frames without invisible pixels
* write background section of disserteation


