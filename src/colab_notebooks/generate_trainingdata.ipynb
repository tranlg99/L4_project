{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script for Generating Training Data from Frames using PIPs\n",
        "https://github.com/aharley/pips"
      ],
      "metadata": {
        "id": "7aDztnqsvgfZ"
      },
      "id": "7aDztnqsvgfZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial set up\n",
        "Connection to MyDrive, requirements, imports, downloading models weights"
      ],
      "metadata": {
        "id": "II3QRFFivngi"
      },
      "id": "II3QRFFivngi"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "239cca92",
      "metadata": {
        "id": "239cca92",
        "outputId": "997f6d32-b99f-4323-ecf3-841e5e1c27f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connecting drive to colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing requirements\n",
        "!pip install imageio==2.19.3\n",
        "!pip install opencv-python==4.6.0.66\n",
        "!pip install einops==0.4.1\n",
        "!pip install scikit-learn==1.1.1\n",
        "!pip install matplotlib==3.5.1\n",
        "!pip install protobuf==3.20.0\n",
        "!pip install fire==0.4.0"
      ],
      "metadata": {
        "id": "daqRwkpfAq6L",
        "outputId": "c9a30241-c2af-42f1-da95-bc8687abdc08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "daqRwkpfAq6L",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio==2.19.3\n",
            "  Downloading imageio-2.19.3-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 26.9 MB/s \n",
            "\u001b[?25hCollecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 68.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from imageio==2.19.3) (1.21.6)\n",
            "Installing collected packages: pillow, imageio\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.9.0\n",
            "    Uninstalling imageio-2.9.0:\n",
            "      Successfully uninstalled imageio-2.9.0\n",
            "Successfully installed imageio-2.19.3 pillow-9.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python==4.6.0.66 in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python==4.6.0.66) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops==0.4.1\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn==1.1.1\n",
            "  Downloading scikit_learn-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.1) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.1) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.1) (1.21.6)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "Successfully installed scikit-learn-1.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.5.1\n",
            "  Downloading matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 33.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1) (1.21.6)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1) (9.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1) (1.15.0)\n",
            "Installing collected packages: fonttools, matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed fonttools-4.38.0 matplotlib-3.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.0\n",
            "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 31.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.57.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.7.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.3.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.16.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\n",
            "Successfully installed protobuf-3.20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fire==0.4.0\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 36.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 44.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 30 kB 53.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 51 kB 32.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 61 kB 37.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 71 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 81 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 87 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fire==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire==0.4.0) (2.1.1)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=dc910fba70716967b8700a6d812e4bdbfd111faf9de6c31e1b566c567126cff3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/10/06/2a990ee4d73a8479fe2922445e8a876d38cfbfed052284c6a1\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/drive_folder/pips')\n",
        "import time\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import saverloader\n",
        "from nets.pips import Pips\n",
        "import utils.improc\n",
        "import random\n",
        "import glob\n",
        "from utils.basic import print_, print_stats\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import imageio.v2 as imageio\n",
        "from csv import writer\n",
        "\n",
        "# save numpy array as npz file\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed"
      ],
      "metadata": {
        "id": "O_R-IzS1EPcy"
      },
      "id": "O_R-IzS1EPcy",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading their reference model\n",
        "!bash /content/drive/MyDrive/drive_folder/pips/get_reference_model.sh"
      ],
      "metadata": {
        "id": "VngwyXNrAuQd",
        "outputId": "c9d72789-f7b6-49ca-acfe-31c665c7cbad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VngwyXNrAuQd",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading the model from dropbox...\n",
            "--2022-12-06 14:08:02--  https://www.dropbox.com/s/hbo7ns4vfx1sejp/reference_model.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/hbo7ns4vfx1sejp/reference_model.tar.gz [following]\n",
            "--2022-12-06 14:08:02--  https://www.dropbox.com/s/raw/hbo7ns4vfx1sejp/reference_model.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc00c292aca090220047539967d4.dl.dropboxusercontent.com/cd/0/inline/ByHCSUaEBJHMqODLe3Hkb1TKAlQHd92iqMDC-1diBYPQXbaBJ7HeTbuTyuOaZMgymWVr-x5x7ab2GA6_FoKbTA6gjiW7vpSqXiGonQQi1k0ULTnaCTG_qLnYZqLS9cESRG2oHlKwrzQLj1ewsAWwCYmKWo9bg_31C9redYprjQ5FQA/file# [following]\n",
            "--2022-12-06 14:08:02--  https://uc00c292aca090220047539967d4.dl.dropboxusercontent.com/cd/0/inline/ByHCSUaEBJHMqODLe3Hkb1TKAlQHd92iqMDC-1diBYPQXbaBJ7HeTbuTyuOaZMgymWVr-x5x7ab2GA6_FoKbTA6gjiW7vpSqXiGonQQi1k0ULTnaCTG_qLnYZqLS9cESRG2oHlKwrzQLj1ewsAWwCYmKWo9bg_31C9redYprjQ5FQA/file\n",
            "Resolving uc00c292aca090220047539967d4.dl.dropboxusercontent.com (uc00c292aca090220047539967d4.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc00c292aca090220047539967d4.dl.dropboxusercontent.com (uc00c292aca090220047539967d4.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/ByGXdqHD4sEVR3mwYpSNbrEDs3M7k2R-RMwigOVknSjIAUZBXqliNouIY7jfPky8WPMR8cssiCBVXTSxLcXcPoF4P4vHMckl4Gk1nXg7Zm-AxoCV1IwgTjbWOMOK7gKPsWUJew-eP5GlDVIpn9pIZ-88AQ-r0A77ELFhJoeKEGcBp04HEezLe2lYo_2Lk99RiHzz6XPGoxb0lq5mrOjTk66kZryMWzjxbOUoNnUE4-sPBh5u_SzFKtrdRw-0ON2seQWsXZoKIfl2faHKT-M2xup9qOPQRqisCZMVj-KfnWxWpys8_zaYGoZAjvhd_wl2CFiS07t0QYWWfOLD8771yoSxclBjPJm8RcCY8WrNT3fDPCMUKJEedPwCwGolvunDdYnwv7diT53WLFhKu-h-RDz7I7mkXeJpUlxv6OV1qAr67Q/file [following]\n",
            "--2022-12-06 14:08:03--  https://uc00c292aca090220047539967d4.dl.dropboxusercontent.com/cd/0/inline2/ByGXdqHD4sEVR3mwYpSNbrEDs3M7k2R-RMwigOVknSjIAUZBXqliNouIY7jfPky8WPMR8cssiCBVXTSxLcXcPoF4P4vHMckl4Gk1nXg7Zm-AxoCV1IwgTjbWOMOK7gKPsWUJew-eP5GlDVIpn9pIZ-88AQ-r0A77ELFhJoeKEGcBp04HEezLe2lYo_2Lk99RiHzz6XPGoxb0lq5mrOjTk66kZryMWzjxbOUoNnUE4-sPBh5u_SzFKtrdRw-0ON2seQWsXZoKIfl2faHKT-M2xup9qOPQRqisCZMVj-KfnWxWpys8_zaYGoZAjvhd_wl2CFiS07t0QYWWfOLD8771yoSxclBjPJm8RcCY8WrNT3fDPCMUKJEedPwCwGolvunDdYnwv7diT53WLFhKu-h-RDz7I7mkXeJpUlxv6OV1qAr67Q/file\n",
            "Reusing existing connection to uc00c292aca090220047539967d4.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 345582080 (330M) [application/octet-stream]\n",
            "Saving to: ‘reference_model.tar.gz’\n",
            "\n",
            "reference_model.tar 100%[===================>] 329.57M   188MB/s    in 1.8s    \n",
            "\n",
            "2022-12-06 14:08:05 (188 MB/s) - ‘reference_model.tar.gz’ saved [345582080/345582080]\n",
            "\n",
            "extracting from tar...\n",
            "reference_model/\n",
            "reference_model/model-000100000.pth\n",
            "deleting the tar...\n",
            "removed 'reference_model.tar.gz'\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data to generate training data from"
      ],
      "metadata": {
        "id": "R9V-6CiZvycm"
      },
      "id": "R9V-6CiZvycm"
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"frames_small\" # specify the name of the data folder located in drive\n",
        "!unzip -d \"$DATA_DIR\"/ /content/drive/MyDrive/\"$DATA_DIR\".zip # unziping frames"
      ],
      "metadata": {
        "id": "lLBW0C-an9sl"
      },
      "id": "lLBW0C-an9sl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating training data"
      ],
      "metadata": {
        "id": "qBTvq9aKwUkL"
      },
      "id": "qBTvq9aKwUkL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PIPs implementation"
      ],
      "metadata": {
        "id": "Rp5xw41PJcEu"
      },
      "id": "Rp5xw41PJcEu"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(model_weights):\n",
        "  model = Pips(stride=4).cuda()\n",
        "  parameters = list(model.parameters())\n",
        "  _ = saverloader.load(model_weights, model)\n",
        "  global_step = 0\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "\n",
        "def run_model(model, rgbs, N, split):\n",
        "    rgbs = rgbs.cuda().float() # B, S, C, H, W\n",
        "\n",
        "    B, S, C, H, W = rgbs.shape\n",
        "    rgbs_ = rgbs.reshape(B*S, C, H, W)\n",
        "    H_, W_ = 360, 640\n",
        "    rgbs_ = F.interpolate(rgbs_, (H_, W_), mode='bilinear')\n",
        "    H, W = H_, W_\n",
        "    rgbs = rgbs_.reshape(B, S, C, H, W)\n",
        "\n",
        "    # pick N points to track; we'll use a uniform grid\n",
        "    N_ = np.sqrt(N).round().astype(np.int32)\n",
        "    grid_y, grid_x = utils.basic.meshgrid2d(B, N_, N_, stack=False, norm=False, device='cuda')\n",
        "    grid_y = 8 + grid_y.reshape(B, -1)/float(N_-1) * (H-16)\n",
        "    grid_x = 8 + grid_x.reshape(B, -1)/float(N_-1) * (W-16)\n",
        "    xy = torch.stack([grid_x, grid_y], dim=-1) # B, N_*N_, 2\n",
        "    _, S, C, H, W = rgbs.shape\n",
        "\n",
        "    print_stats('rgbs', rgbs)\n",
        "\n",
        "    # splitting grid of points into tensors of size 1500 along dim=1\n",
        "    if split:\n",
        "       xy_split = torch.split(xy, 1500, dim=1)\n",
        "       preds_split = []\n",
        "\n",
        "      # for each splitted point compute trajs\n",
        "       for i in range(len(xy_split)):\n",
        "         preds, preds_anim, vis_e, stats = model(xy_split[i], rgbs, iters=6)      \n",
        "         preds_split.append(preds[-1])\n",
        "\n",
        "       # put trajs back together\n",
        "       trajs_e = torch.cat(preds_split, 2)\n",
        "       \n",
        "\n",
        "    else:\n",
        "      preds, preds_anim, vis_e, stats = model(xy, rgbs, iters=6)\n",
        "      # preds is a list of torch tensors \n",
        "      trajs_e = preds[-1] # tensor of shape (frames, tracking points, 2)\n",
        "    print_stats('trajs_e', trajs_e)\n",
        "\n",
        "    # return vis?? as well\n",
        "    return trajs_e[:,-1,:,:]\n",
        "\n",
        "def generate_training_data(pips_model, video_name, n, split=True):\n",
        "    model = pips_model\n",
        "    \n",
        "    ## Choose hyps\n",
        "    B = 1\n",
        "    S = 8\n",
        "    N = n**2 # number of points to track\n",
        "\n",
        "    # Get frames of a file\n",
        "    filenames = glob.glob(DATA_DIR+'/content/frames/'+video_name+'/frames/*.jpg')\n",
        "    filenames = sorted(filenames)\n",
        "\n",
        "    # Remove every second frame from a list\n",
        "    n = 2\n",
        "    del filenames[n - 1::n]\n",
        "\n",
        "    max_iters = len(filenames)//S # run each unique subsequence\n",
        "    global_step = 0\n",
        "\n",
        "    # Run model each of 8 frames\n",
        "    while global_step < max_iters:\n",
        "        global_step += 1\n",
        "        \n",
        "        try:\n",
        "            rgbs = []\n",
        "            sample_id = video_name[-11:]+\"_\"+str((global_step-1)*S)\n",
        "            print(\"sample {}: step {}/{}\".format(sample_id, global_step, max_iters))\n",
        "\n",
        "            # skip generating this sample if already in the log list\n",
        "            if sample_id in LOG_LIST:\n",
        "                  print(sample_id+\" already in log list\")\n",
        "                  continue\n",
        "            for s in range(S):\n",
        "                frame_num = (global_step-1)*S+s\n",
        "                fn = filenames[frame_num]\n",
        "                if s==0:\n",
        "                    print('start frame', fn)\n",
        "                im = imageio.imread(fn)\n",
        "                im = im.astype(np.uint8)\n",
        "                im = torch.from_numpy(im).permute(2,0,1)\n",
        "                rgbs.append(im)\n",
        "            rgbs = torch.stack(rgbs, dim=0).unsqueeze(0) # 1, S, C, H, W\n",
        "\n",
        "            with torch.no_grad():\n",
        "                trajs_e = run_model(model, rgbs, N, split)\n",
        "\n",
        "            # store img0 and img1 (frame 1 and 8) and trajs_e\n",
        "            save_data(sample_id, rgbs[0][0], rgbs[0][-1], trajs_e)\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print('error', e)\n",
        "\n",
        "def img_to_array(tensor_img):\n",
        "   x = tensor_img.permute(1, 2, 0).numpy() # from tensor to numpy array\n",
        "   x = cv2.resize(x, (240, 120)) # resizing image\n",
        "   return x\n",
        "\n",
        "def save_data(sample_id, frame0, frame7, trajs):\n",
        "  \"\"\"\n",
        "  Saving coordinates to or COORDS_DIR as .npy files or all together as CSV_FILE file???\n",
        "  Saving 0th frames to FRAME0_DIR directory as .npy files\n",
        "  Saving 7th frames to FRAME7_DIR directory as .npy files\n",
        "  - Coordinates and frames are mapped with sample_id, all sample_ids are stored in sample_ids.txt\n",
        "  \"\"\"\n",
        "  img0 = img_to_array(frame0) # transform first frame to array\n",
        "  img7 = img_to_array(frame7) # tranform last frame (8th) to array\n",
        "  coords = trajs.cpu().numpy()\n",
        "\n",
        "  # Store coords, frame0, frame1\n",
        "  np.save(COORDS_DIR+sample_id, coords)\n",
        "  np.save(FRAME0_DIR+sample_id, img0)\n",
        "  np.save(FRAME1_DIR+sample_id, img7)\n",
        "\n",
        "  # Add sample_id to log list\n",
        "  LOG_LIST.append(sample_id)\n",
        "\n",
        "def save_log(log_list):\n",
        "  with open(LOG_FILE, \"w\") as outfile:\n",
        "    outfile.write(\"\\n\".join(log_list))"
      ],
      "metadata": {
        "id": "jWzwnNKZKFyj"
      },
      "id": "jWzwnNKZKFyj",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model('reference_model')"
      ],
      "metadata": {
        "id": "LTZQnKzeKl6r",
        "outputId": "50ed4bd5-7501-4569-c9b5-57e3032634b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LTZQnKzeKl6r",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading ckpt from reference_model\n",
            "...found checkpoint reference_model/model-000100000.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating directories and running generation of data"
      ],
      "metadata": {
        "id": "YYg8VkJJJh0P"
      },
      "id": "YYg8VkJJJh0P"
    },
    {
      "cell_type": "code",
      "source": [
        "#create a folder to store training data in\n",
        "!mkdir training_data\n",
        "!mkdir training_data/frame0/\n",
        "!mkdir training_data/frame1/\n",
        "!mkdir training_data/coords/\n",
        "!touch training_data/sample_ids.txt\n",
        "PATH = \"\""
      ],
      "metadata": {
        "id": "4cZ_q9I6tBNj"
      },
      "id": "4cZ_q9I6tBNj",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if training data folder already exists cp from drive to colab memory and unzip\n",
        "TRAIN_DATA_DIR = \"training_data_01122022\" # specify the name of the data folder located in drive\n",
        "!unzip -d \"$TRAIN_DATA_DIR\"/ /content/drive/MyDrive/\"$TRAIN_DATA_DIR\".zip # unziping frames\n",
        "PATH = \"training_data_01122022/content/\""
      ],
      "metadata": {
        "id": "tApdIoeTvoDg"
      },
      "id": "tApdIoeTvoDg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANING_DATA_DIR = PATH+\"training_data/\"\n",
        "COORDS_DIR = PATH+\"training_data/coords/\"\n",
        "FRAME0_DIR= PATH+\"training_data/frame0/\"\n",
        "FRAME1_DIR= PATH+\"training_data/frame1/\"\n",
        "LOG_FILE = PATH+\"training_data/sample_ids.txt\"\n",
        "LOG_LIST = open(LOG_FILE).read().splitlines()"
      ],
      "metadata": {
        "id": "lyp0MVe7uWwf"
      },
      "id": "lyp0MVe7uWwf",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_training_data(model, 'Tai chi56式夕陽美功夫扇-uOw-z7CR7x8', n=64)\n",
        "save_log(LOG_LIST)"
      ],
      "metadata": {
        "id": "lkF4vYMA7zvM"
      },
      "id": "lkF4vYMA7zvM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of file names to go through\n",
        "videos_list = [f for f in os.listdir('frames_small/content/frames/') if not f.startswith('.')]\n",
        "print(videos_list)"
      ],
      "metadata": {
        "id": "_DmrwfqCnJVH",
        "outputId": "863dad55-92ef-4f4c-bc8a-70114c4d48cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_DmrwfqCnJVH",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tai chiRen Guang Yi - Chen Style Taiji silk reeling part II-Wh3AmDsdQtM', 'Tai chiCanda - Tai Chi Chuan Yang-Stil - Sanfte Bewegungsformen für Einsteiger-f7NkWPgh1-o', 'Tai chi陈式太极拳五十六式-vB8XTJfV4rY', 'Tai chi56式夕陽美功夫扇-uOw-z7CR7x8', 'Tai chiShaolin Basics Are From Theater!-gyms4lomW50', 'Tai chiWee Kee Jin - Keeping Your Structure-7jn9jeAbChE', 'Tai chiYang Family Tai Chi q&a Knee Brush-WPeVwAhTNuU', 'Tai chi熊門楊家太極拳111式第一段\\u3000李國光老師示範-X_9SJZuSWQU', 'Tai chi10 forms 2009-KX-dEeB47sc', 'Tai chiCurso Chi Kung de los Seis Sonidos Curativos-PvjYVsRK4Dg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate through the videos list and generate training data files\n",
        "for x in range(len(videos_list)):\n",
        "  print(\"Now generating training data for {}, {} out of {}.\".format(videos_list[x], x, len(videos_list)))\n",
        "  generate_training_data(model, videos_list[x], 100)"
      ],
      "metadata": {
        "id": "6H_eyv85wZd3"
      },
      "id": "6H_eyv85wZd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zip data and store in Drive"
      ],
      "metadata": {
        "id": "y7K9XdyxJwAc"
      },
      "id": "y7K9XdyxJwAc"
    },
    {
      "cell_type": "code",
      "source": [
        "# zip training data\n",
        "!zip -r /content/training_data_06122022.zip /content/training_data -x \"*/.*\"\n",
        "# cp NumPy zip file into drive\n",
        "!cp training_data_06122022.zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "PH9XF70MulWV"
      },
      "id": "PH9XF70MulWV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "W5KEucYqJUXz"
      },
      "id": "W5KEucYqJUXz"
    },
    {
      "cell_type": "code",
      "source": [
        "# see training data of one video for verification\n",
        "from numpy import load\n",
        "data = load('training_data/coords/Wh3AmDsdQtM0.npy', allow_pickle=True)\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "ZAJ1hanTBl2C",
        "outputId": "637befcd-22b8-45f1-c899-8ba58dab487c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZAJ1hanTBl2C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10000, 2)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}