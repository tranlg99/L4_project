{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script for Generating Training Data from Frames using PIPs\n",
        "https://github.com/aharley/pips"
      ],
      "metadata": {
        "id": "7aDztnqsvgfZ"
      },
      "id": "7aDztnqsvgfZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial set up\n",
        "Connection to MyDrive, requirements, imports, downloading models weights"
      ],
      "metadata": {
        "id": "II3QRFFivngi"
      },
      "id": "II3QRFFivngi"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "239cca92",
      "metadata": {
        "id": "239cca92",
        "outputId": "374d03a1-096b-4d38-c78b-172f5ea7e9a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# connecting drive to colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing requirements\n",
        "!pip install imageio==2.19.3\n",
        "!pip install opencv-python==4.6.0.66\n",
        "!pip install einops==0.4.1\n",
        "!pip install scikit-learn==1.1.1\n",
        "!pip install matplotlib==3.5.1\n",
        "!pip install protobuf==3.20.0\n",
        "!pip install fire==0.4.0"
      ],
      "metadata": {
        "id": "daqRwkpfAq6L"
      },
      "id": "daqRwkpfAq6L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/drive_folder/pips')\n",
        "import time\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import saverloader\n",
        "from nets.pips import Pips\n",
        "import utils.improc\n",
        "import random\n",
        "import glob\n",
        "from utils.basic import print_, print_stats\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import imageio.v2 as imageio\n",
        "from csv import writer\n",
        "# imports pytorch\n",
        "import torch"
      ],
      "metadata": {
        "id": "O_R-IzS1EPcy"
      },
      "id": "O_R-IzS1EPcy",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading their reference model\n",
        "!bash /content/drive/MyDrive/drive_folder/pips/get_reference_model.sh"
      ],
      "metadata": {
        "id": "VngwyXNrAuQd",
        "outputId": "2b20788a-4e16-4e47-cd0c-6d7f2ac01069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VngwyXNrAuQd",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading the model from dropbox...\n",
            "--2023-02-18 08:29:40--  https://www.dropbox.com/s/hbo7ns4vfx1sejp/reference_model.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.84.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.84.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/hbo7ns4vfx1sejp/reference_model.tar.gz [following]\n",
            "--2023-02-18 08:29:41--  https://www.dropbox.com/s/raw/hbo7ns4vfx1sejp/reference_model.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1f35238a19f13d887644c7eba1.dl.dropboxusercontent.com/cd/0/inline/B2vRfUCWhLxnUE79btxaZltOjDQ1vuWczMQBUShM2WYAJTTJC5gVVCLeo20hM9iQdfCcQlsTJvkai9arA5AAskp08Lbqedg0QhIorGHVv5CLEk8IH7akWrkJZpDLOLXhds7-3-SdvoDorwGEhDmK2IjtCZez1DIouasfHUOOSt3BnA/file# [following]\n",
            "--2023-02-18 08:29:41--  https://uc1f35238a19f13d887644c7eba1.dl.dropboxusercontent.com/cd/0/inline/B2vRfUCWhLxnUE79btxaZltOjDQ1vuWczMQBUShM2WYAJTTJC5gVVCLeo20hM9iQdfCcQlsTJvkai9arA5AAskp08Lbqedg0QhIorGHVv5CLEk8IH7akWrkJZpDLOLXhds7-3-SdvoDorwGEhDmK2IjtCZez1DIouasfHUOOSt3BnA/file\n",
            "Resolving uc1f35238a19f13d887644c7eba1.dl.dropboxusercontent.com (uc1f35238a19f13d887644c7eba1.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc1f35238a19f13d887644c7eba1.dl.dropboxusercontent.com (uc1f35238a19f13d887644c7eba1.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B2upPMdsMNvImmN1R5tb6FTPLBtGroUzU1eWiH7sszxO2mgbMPF662JYSpYMkq-DXrNMLA5yjAPxsfBR9iphZUpTsW-ebVliQIVT_7NJH2VDpZ54iZoXK4rV1YRh-Tjq70mv4-4Irh5Vxf0_xKQlL4HVm-jHs53MhTtoXyKja-BFOx-XCu71HNKZXsiujzCLR_yx0Sc0NcZ9BucdB0peHzzaSo3SvcGkrAn1V0oGhikeWOPCesqt03UPajFBROfbdYS_eqivDFragHO6xaLwhufRRUyLuxVZVqUz544daDo2zchuoZHxKjvVwOsHpocM9C-PTpe-w33qUgUnbY9jNYau02sEUkB9qC4NTMypX982m04OKeFcReCcflTiJgrauPbrOtZH4wxtjOCmjq-FcWH-b-ofQaSPDfl-eihyA_xbdQ/file [following]\n",
            "--2023-02-18 08:29:43--  https://uc1f35238a19f13d887644c7eba1.dl.dropboxusercontent.com/cd/0/inline2/B2upPMdsMNvImmN1R5tb6FTPLBtGroUzU1eWiH7sszxO2mgbMPF662JYSpYMkq-DXrNMLA5yjAPxsfBR9iphZUpTsW-ebVliQIVT_7NJH2VDpZ54iZoXK4rV1YRh-Tjq70mv4-4Irh5Vxf0_xKQlL4HVm-jHs53MhTtoXyKja-BFOx-XCu71HNKZXsiujzCLR_yx0Sc0NcZ9BucdB0peHzzaSo3SvcGkrAn1V0oGhikeWOPCesqt03UPajFBROfbdYS_eqivDFragHO6xaLwhufRRUyLuxVZVqUz544daDo2zchuoZHxKjvVwOsHpocM9C-PTpe-w33qUgUnbY9jNYau02sEUkB9qC4NTMypX982m04OKeFcReCcflTiJgrauPbrOtZH4wxtjOCmjq-FcWH-b-ofQaSPDfl-eihyA_xbdQ/file\n",
            "Reusing existing connection to uc1f35238a19f13d887644c7eba1.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 345582080 (330M) [application/octet-stream]\n",
            "Saving to: ‘reference_model.tar.gz’\n",
            "\n",
            "reference_model.tar 100%[===================>] 329.57M  17.0MB/s    in 23s     \n",
            "\n",
            "2023-02-18 08:30:06 (14.4 MB/s) - ‘reference_model.tar.gz’ saved [345582080/345582080]\n",
            "\n",
            "extracting from tar...\n",
            "reference_model/\n",
            "reference_model/model-000100000.pth\n",
            "deleting the tar...\n",
            "removed 'reference_model.tar.gz'\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading zip folder with data frames\n",
        "used to compute particle flow"
      ],
      "metadata": {
        "id": "R9V-6CiZvycm"
      },
      "id": "R9V-6CiZvycm"
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"frames\" # specify the name of the data folder located in drive\n",
        "!unzip -d \"$DATA_DIR\"/ /content/drive/MyDrive/\"$DATA_DIR\".zip # unziping frames"
      ],
      "metadata": {
        "id": "lLBW0C-an9sl"
      },
      "id": "lLBW0C-an9sl",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of videos to go through\n",
        "videos_list = [f for f in os.listdir(DATA_DIR+'/content/frames/') if not f.startswith('.')]\n",
        "print(videos_list)"
      ],
      "metadata": {
        "id": "_DmrwfqCnJVH"
      },
      "id": "_DmrwfqCnJVH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating training data"
      ],
      "metadata": {
        "id": "qBTvq9aKwUkL"
      },
      "id": "qBTvq9aKwUkL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "3IobyVPxiz-c"
      },
      "id": "3IobyVPxiz-c"
    },
    {
      "cell_type": "code",
      "source": [
        "def img_to_array(tensor_img):\n",
        "  \"\"\"\n",
        "  Change tensor to numpy array for storing of images\n",
        "  - resizing to 640x360 (to match coords resolution)\n",
        "  - normalising pixel value to range 0-1 (required for input to FCN ResNet-50)\n",
        "  \"\"\"\n",
        "  x = tensor_img.permute(1, 2, 0).numpy()\n",
        "  x = cv2.resize(x, (640, 360))\n",
        "  x = x.astype(float) / 255\n",
        "  return x\n",
        "\n",
        "def save_data(sample_id, frame0, frame3, frame7, trajs, vis):\n",
        "  \"\"\"\n",
        "  Saving\n",
        "  - coordinates to COORDS_DIR as .npy files\n",
        "  - visibility of coordinates to VIS_DIR as .npy files\n",
        "  - 0th frames to FRAME0_DIR directory as .npy files\n",
        "  - 7th frames to FRAME7_DIR directory as .npy files\n",
        "  \n",
        "  - Coordinates, visibility and frames are mapped with sample_id, all sample_ids are stored in sample_ids.txt\n",
        "  \"\"\"\n",
        "  img0 = img_to_array(frame0) # transform 0th frame to array\n",
        "  img3 = img_to_array(frame3) # transform 2nd frame to array\n",
        "  img7 = img_to_array(frame7) # tranform last frame (8th) to array\n",
        "  coords = trajs.cpu().numpy()\n",
        "  vis = vis.cpu().numpy()\n",
        "\n",
        "  # Store coords, frame0, frame1\n",
        "  np.save(COORDS_DIR+sample_id, coords)\n",
        "  np.save(VIS_DIR+sample_id, vis)\n",
        "  np.save(FRAME0_DIR+sample_id, img0)\n",
        "  np.save(FRAME3_DIR+sample_id, img3)\n",
        "  np.save(FRAME7_DIR+sample_id, img7)\n",
        "\n",
        "  # Add sample_id to log list\n",
        "  LOG_LIST.append(sample_id)\n",
        "  save_log(LOG_LIST)\n",
        "\n",
        "def save_log(log_list):\n",
        "  with open(LOG_FILE, \"w\") as outfile:\n",
        "    outfile.write(\"\\n\".join(log_list))"
      ],
      "metadata": {
        "id": "7JZqCwd8iw4D"
      },
      "id": "7JZqCwd8iw4D",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PIPs implementation"
      ],
      "metadata": {
        "id": "Rp5xw41PJcEu"
      },
      "id": "Rp5xw41PJcEu"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(model_weights):\n",
        "  model = Pips(stride=4).cuda()\n",
        "  parameters = list(model.parameters())\n",
        "  _ = saverloader.load(model_weights, model)\n",
        "  global_step = 0\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "def run_model(model, rgbs, N, split):\n",
        "    rgbs = rgbs.cuda().float() # B, S, C, H, W\n",
        "\n",
        "    B, S, C, H, W = rgbs.shape\n",
        "    rgbs_ = rgbs.reshape(B*S, C, H, W)\n",
        "    H_, W_ = 360, 640\n",
        "    rgbs_ = F.interpolate(rgbs_, (H_, W_), mode='bilinear')\n",
        "    H, W = H_, W_\n",
        "    rgbs = rgbs_.reshape(B, S, C, H, W)\n",
        "\n",
        "    # Pick N points to track; we'll use a uniform grid\n",
        "    N_ = np.sqrt(N).round().astype(np.int32)\n",
        "    grid_y, grid_x = utils.basic.meshgrid2d(B, N_, N_, stack=False, norm=False, device='cuda')\n",
        "    grid_y = 8 + grid_y.reshape(B, -1)/float(N_-1) * (H-16)\n",
        "    grid_x = 8 + grid_x.reshape(B, -1)/float(N_-1) * (W-16)\n",
        "    xy = torch.stack([grid_x, grid_y], dim=-1) # B, N_*N_, 2\n",
        "    _, S, C, H, W = rgbs.shape\n",
        "\n",
        "    print_stats('rgbs', rgbs)\n",
        "\n",
        "    # Splitting grid of points into tensors of size 1500 along dim=1\n",
        "    if split:\n",
        "       xy_split = torch.split(xy, 1500, dim=1)\n",
        "       preds_split = []\n",
        "       vis_split = []\n",
        "\n",
        "      # For each splitted point compute trajs\n",
        "       for i in range(len(xy_split)):\n",
        "         preds, preds_anim, vis, stats = model(xy_split[i], rgbs, iters=6)\n",
        "         preds_split.append(preds[-1])\n",
        "         vis_split.append(vis[-1])\n",
        "\n",
        "       # Put trajs back together\n",
        "       trajs_e = torch.cat(preds_split, 2)\n",
        "       vis_e = torch.cat(vis_split, 1).unsqueeze(0)\n",
        "       \n",
        "\n",
        "    else:\n",
        "      preds, preds_anim, vis, stats = model(xy, rgbs, iters=6)\n",
        "      # preds is a list of torch tensors \n",
        "      trajs_e = preds[-1] # tensor of shape (frames, tracking points, 2)\n",
        "      vis_e = vis\n",
        "\n",
        "    print_stats('vis_e', vis_e)\n",
        "    return trajs_e[:,-1,:,:], vis_e[:,-1,:]\n",
        "\n",
        "def generate_training_data(pips_model, video_name, n, split=True):\n",
        "    model = pips_model\n",
        "    \n",
        "    # Choose hyps\n",
        "    B = 1\n",
        "    S = 8\n",
        "    N = n**2 # number of points to track\n",
        "\n",
        "    # Get frames of a file\n",
        "    filenames = glob.glob(DATA_DIR+'/content/frames/'+video_name+'/frames/*.jpg')\n",
        "    filenames = sorted(filenames)\n",
        "\n",
        "    # Remove every second frame from a list\n",
        "    n = 2\n",
        "    del filenames[n - 1::n]\n",
        "\n",
        "    max_iters = (len(filenames)//S)-100 # run each unique subsequence\n",
        "    global_step = 100\n",
        "\n",
        "    # Run model each of 8 frames, from frame 100 to frame end-100 (to avoid titles in the video)\n",
        "    while global_step < max_iters:\n",
        "        global_step += 1\n",
        "        \n",
        "        try:\n",
        "            rgbs = []\n",
        "            sample_id = video_name[-11:]+\"_\"+str((global_step-1)*S)\n",
        "            print(\"sample {}: step {}/{}\".format(sample_id, global_step, max_iters))\n",
        "\n",
        "            # Skip generating this sample if it is already in the log list\n",
        "            if sample_id in LOG_LIST:\n",
        "                  print(sample_id+\" already in log list\")\n",
        "                  continue\n",
        "\n",
        "            for s in range(S):\n",
        "                frame_num = (global_step-1)*S+s\n",
        "                fn = filenames[frame_num]\n",
        "                if s==0:\n",
        "                    print('start frame', fn)\n",
        "                im = imageio.imread(fn)\n",
        "                im = im.astype(np.uint8)\n",
        "                im = torch.from_numpy(im).permute(2,0,1)\n",
        "                rgbs.append(im)\n",
        "            rgbs = torch.stack(rgbs, dim=0).unsqueeze(0) # 1, S, C, H, W\n",
        "\n",
        "            with torch.no_grad():\n",
        "                trajs_e, vis_e = run_model(model, rgbs, N, split)\n",
        "\n",
        "            # Store img0, img3 and img7 (first, middle and last frame) and trajs_e, vis_e\n",
        "            save_data(sample_id, rgbs[0][0], rgbs[0][3], rgbs[0][-1], trajs_e, vis_e)\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print('error', e)"
      ],
      "metadata": {
        "id": "jWzwnNKZKFyj"
      },
      "id": "jWzwnNKZKFyj",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model('reference_model')"
      ],
      "metadata": {
        "id": "LTZQnKzeKl6r",
        "outputId": "c3807c42-a7c2-4a55-ef06-e9a1d80dc7c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LTZQnKzeKl6r",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading ckpt from reference_model\n",
            "...found checkpoint reference_model/model-000100000.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating/Loading folder to store data in"
      ],
      "metadata": {
        "id": "YYg8VkJJJh0P"
      },
      "id": "YYg8VkJJJh0P"
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell if you want to create a new dataset,\n",
        "# will create a folder to store training data in\n",
        "!rm -r training_data\n",
        "!mkdir training_data\n",
        "!mkdir training_data/frame0/\n",
        "!mkdir training_data/frame3/\n",
        "!mkdir training_data/frame7/\n",
        "!mkdir training_data/coords/\n",
        "!mkdir training_data/vis/\n",
        "!touch training_data/sample_ids.txt\n",
        "PATH = \"/content/training_data\""
      ],
      "metadata": {
        "id": "4cZ_q9I6tBNj",
        "outputId": "2881c6ea-7a43-49fd-9a14-6b9493e7edef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4cZ_q9I6tBNj",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'training_data': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell if we want to use an old training data folder\n",
        "# specify the name of the data folder located in drive\n",
        "TRAIN_DATA_DIR = \"full_dataset_final__2023-02-17\"\n",
        "!unzip -d \"$TRAIN_DATA_DIR\"/ /content/drive/MyDrive/\"$TRAIN_DATA_DIR\".zip\n",
        "\n",
        "# Set path to that folder\n",
        "PATH = \"/content/\"+TRAIN_DATA_DIR"
      ],
      "metadata": {
        "id": "O1J89KpEaKfw"
      },
      "id": "O1J89KpEaKfw",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANING_DATA_DIR = PATH+\"/\"\n",
        "COORDS_DIR = PATH+\"/coords/\"\n",
        "VIS_DIR = PATH+\"/vis/\"\n",
        "FRAME0_DIR= PATH+\"/frame0/\"\n",
        "FRAME3_DIR= PATH+\"/frame3/\"\n",
        "FRAME7_DIR= PATH+\"/frame7/\"\n",
        "LOG_FILE = PATH+\"/sample_ids.txt\"\n",
        "LOG_LIST = open(LOG_FILE).read().splitlines()"
      ],
      "metadata": {
        "id": "lyp0MVe7uWwf"
      },
      "id": "lyp0MVe7uWwf",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deleting corrupted files"
      ],
      "metadata": {
        "id": "KB1uI5VXuz4o"
      },
      "id": "KB1uI5VXuz4o"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fnmatch\n",
        "corrupted = [\"-CR4xjdQbkc\", \"MGYF1aDwUKg\", \"vB8XTJfV4rY\", \"KjMxdYJOwqI\", \"asL3ZyuNeB0\", \"KMO1BluPtU4\", \"sm6-dbG5Rho\"]\n",
        "folders = ['/frame0/', '/frame3/', '/frame7/', '/coords/', '/vis/']\n",
        "for c in corrupted:\n",
        "  print(c)\n",
        "  for folder in folders:\n",
        "    for file in os.listdir(PATH+folder):\n",
        "      if fnmatch.fnmatch(file, c+'*.npy'):\n",
        "        os.remove(PATH+folder+'/'+file)"
      ],
      "metadata": {
        "id": "b4yi7tc_qOGi"
      },
      "id": "b4yi7tc_qOGi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating data"
      ],
      "metadata": {
        "id": "LxUcvQog0_ke"
      },
      "id": "LxUcvQog0_ke"
    },
    {
      "cell_type": "code",
      "source": [
        "videos_part1 = ['Tai chiKunlun Si Xiang Quan Demonstrated by Huang Shuanqing--CR4xjdQbkc', #corrupted\n",
        "                'Tai chi陈式太极拳五十六式-vB8XTJfV4rY', # corrupted\n",
        "                'Tai chiTrần thức giản hóa do Vs - Thiều Ngọc Sơn diễn luyện.-MGYF1aDwUKg', #corrupted\n",
        "                'Tai chiYang Taijiquan 16 vorm-KjMxdYJOwqI', #corrupted\n",
        "                ]\n",
        "\n",
        "for v in videos_part1:\n",
        "  print(\"Now generating training data for {}.\".format(v))\n",
        "  generate_training_data(model, v, n=64)"
      ],
      "metadata": {
        "id": "HNvo9dPWXfmz"
      },
      "id": "HNvo9dPWXfmz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos_part2 = ['Tai chiTai Chi 32 Yang Stil Schwertform  _ 32 Step Yang Style Tai Chi Sword Routine-asL3ZyuNeB0', #corrupted,\n",
        "                'Tai chi吳式太極快架-KMO1BluPtU4', # corrupted\n",
        "                'Tai chiTai Chi sword form-sm6-dbG5Rho' #corrupted\n",
        "                ]\n",
        "\n",
        "for v in videos_part2:\n",
        "  print(\"Now generating training data for {}.\".format(v))\n",
        "  generate_training_data(model, v, n=64)"
      ],
      "metadata": {
        "id": "Z6DwMDrMnmH-"
      },
      "id": "Z6DwMDrMnmH-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos_part3 = ['Tai chizhaobao Taichi 24 刘钢 示范 武当赵堡和式太极拳二十四式简化太极拳-FmJ33H476pI',\n",
        "                'Tai chi56式夕陽美功夫扇-uOw-z7CR7x8',\n",
        "                'Tai chiTai Chi for Osteoporosis Front Demo-AUzpWyGi8Gw',\n",
        "                'Tai chichen 56-l0aKiGrTUGw',\n",
        "                'Tai chi陳氏太極拳：老架一路(上)王西安.flv-CA9-8CydW4A',\n",
        "                'Tai chiTaijiquan Tai Chi Pai Lin part 2-6f6nS3PCeQM'\n",
        "                ]\n",
        "\n",
        "for v in videos_part3:\n",
        "  print(\"Now generating training data for {}.\".format(v))\n",
        "  generate_training_data(model, v, n=64)"
      ],
      "metadata": {
        "id": "0r05ihBqoVfE"
      },
      "id": "0r05ihBqoVfE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos_part4 = [\n",
        "                'Tai chiSun style - short form (30 forms)-FzJJ_bWLzcA',\n",
        "                \"Tai chiTai Chi Chuan -Yang Style - Grand Master William C.C. Chen's 60 Revolving Movements - 1st Section-DXO_BbscaFg\",\n",
        "                'Tai chiGuang Ping Yang Tai Ji Quan (Tai Chi Chuan)-VidZQ6yA7I4']\n",
        "\n",
        "for v in videos_part4:\n",
        "  print(\"Now generating training data for {}.\".format(v))\n",
        "  generate_training_data(model, v, n=64)"
      ],
      "metadata": {
        "id": "iHjZhDU6ymsK"
      },
      "id": "iHjZhDU6ymsK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos_part5 = [\n",
        "                'Tai chiTai Chi Chuan-vFDuL2-zhvQ',\n",
        "                'Tai chi陳式太極拳56式 - 游振芳 吳志芬-ypIdWCYjHuo',\n",
        "                'Tai chiLiu He Ba Fa performed by Paul Dillon-BKLJ4mRzYuE',\n",
        "                'Tai chiInstitut Français de Taï Ji Zhang Dongwu-jFuiqvVaESE']\n",
        "\n",
        "for v in videos_part5:\n",
        "  print(\"Now generating training data for {}.\".format(v))\n",
        "  generate_training_data(model, v, n=64)"
      ],
      "metadata": {
        "id": "5bEKAsu5pKe1"
      },
      "id": "5bEKAsu5pKe1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip data and store in Drive"
      ],
      "metadata": {
        "id": "y7K9XdyxJwAc"
      },
      "id": "y7K9XdyxJwAc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check data was generated correctly"
      ],
      "metadata": {
        "id": "IkMNTxSiT69c"
      },
      "id": "IkMNTxSiT69c"
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "TODAY = date.today()\n",
        "FILENAME = \"full_dataset\"\n",
        "\n",
        "# zip training data\n",
        "!cd \"$PATH\" && zip -r /content/\"$FILENAME\"_\"$TODAY\".zip .\n",
        "# cp NumPy zip file into drive\n",
        "!cp \"$FILENAME\"_\"$TODAY\".zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "PH9XF70MulWV"
      },
      "id": "PH9XF70MulWV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fnmatch\n",
        "\n",
        "x = [f for f in os.listdir(PATH+'/coords/') if not f.startswith('.')] \n",
        "for name in x:\n",
        "  if fnmatch.fnmatch(name, 'ypIdWCYjHuo'+'*.npy'):\n",
        "     print(name, np.load(PATH+'/coords/'+name, encoding='bytes').shape)\n",
        "     coords = np.load(PATH+'/coords/'+name, encoding='bytes')\n",
        "     coords = coords.squeeze()\n",
        "     plt.scatter(coords[:, 0], coords[:, 1], s=0.5, marker='.', cmap=plt.cm.coolwarm)\n",
        "     plt.ylim(max(plt.ylim()), min(plt.ylim()))\n",
        "     plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "metadata": {
        "id": "W2w3Cmkin5hJ"
      },
      "id": "W2w3Cmkin5hJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}