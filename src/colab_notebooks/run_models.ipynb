{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxnvQRy/JR+fhiZo/HcFhG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Running prediction models"
      ],
      "metadata": {
        "id": "1I_WGBF8ed6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "zIn87ix6kQsx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Datasets"
      ],
      "metadata": {
        "id": "CnemLliSej6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3913UDRdr6b"
      },
      "outputs": [],
      "source": [
        "# Download synthetic test dataset\n",
        "# !gdown 1-AtP2n5N0J7XTRzPlOz95eomtsLbuyvR -O synthetic_data.zip\n",
        "# !unzip -d synthetic_data/ synthetic_data.zip # unziping test data\n",
        "\n",
        "# Download natural test dataset\n",
        "!gdown 1-0aDjjgh0RCRlWFYmNxIApk-wJ6p8mq_ -O natural_data.zip\n",
        "!unzip -d natural_data/ natural_data.zip # unziping test data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download models"
      ],
      "metadata": {
        "id": "sLIGOVrzenRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model 1A\n",
        "!gdown 1T-NFKCLVBlSkbuR-eai4_fejw7AtslBK\n",
        "# Download model 1B\n",
        "!gdown 1MOeTTWnkc-vYj5Q16ubQIx0yZSRlA8Pi\n",
        "# Download model 1C\n",
        "\n",
        "# Download model 1A\n",
        "\n",
        "# Download model 2B\n",
        "\n",
        "# Download model 2C"
      ],
      "metadata": {
        "id": "IWwTyKc4el_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class"
      ],
      "metadata": {
        "id": "_NRCwxqwf0s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"Our custom dataset.\"\"\"\n",
        "    def __init__(self,log_file,root_dir,check=False,transform=None):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "\t\t    log_file (string): path to txt file with all logged sample ids. \n",
        "\t\t    root_dir (string): Directory with all the image frames.\n",
        "            check (Bool, optional): Also return 7th frames for sanity check.\n",
        "\t\t    transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.sample_names=open(log_file).read().splitlines()\n",
        "        self.root_dir=root_dir\n",
        "        self.check = check\n",
        "        self.transform=transform\n",
        "    def __len__(self):\n",
        "        return len(self.sample_names)\n",
        "    def __getitem__(self,idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx=idx.tolist()\n",
        "        sample_id = self.sample_names[idx]\n",
        "        image0_path=os.path.join(self.root_dir,\"frame0\",sample_id)\n",
        "        image3_path=os.path.join(self.root_dir,\"frame3\",sample_id)\n",
        "        coords_path=os.path.join(self.root_dir,\"coords\",sample_id)\n",
        "        vis_path=os.path.join(self.root_dir,\"vis\",sample_id)\n",
        "\t\n",
        "        image0=np.load(image0_path+'.npy')\n",
        "        image3=np.load(image3_path+'.npy')\n",
        "        image0 = cv2.resize(image0, dsize=(640, 360))\n",
        "        image3 = cv2.resize(image3, dsize=(640, 360))\n",
        "\n",
        "\n",
        "        coords= np.load(coords_path+'.npy')\n",
        "        vis=np.load(vis_path+'.npy')\n",
        "        \n",
        "\n",
        "        if self.check:\n",
        "            image7_path=os.path.join(self.root_dir,\"frame7\",sample_id)\n",
        "            image7=np.load(image7_path+'.npy')\n",
        "            image7 = cv2.resize(image7, dsize=(640, 360))\n",
        "            sample={'id':sample_id, 'image0':image0, 'image3':image3, 'image7':image7, 'coords':coords, 'vis':vis, 'shift':np.array((0,0))}\n",
        "        else:\n",
        "          sample={'id':sample_id, 'image0':image0, 'image3':image3, 'coords':coords, 'vis':vis, 'shift':np.array((0,0))}\n",
        "             \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"\n",
        "    Convert ndarrays in sample to Tensors.\n",
        "    - swap color axis because\n",
        "    - numpy image: H x W x C\n",
        "    - torch image: C x H x W\n",
        "    \"\"\"\n",
        "    def __call__(self, sample):\n",
        "        sample_id, image0, image3, coords, vis, shift = sample['id'], sample['image0'], sample['image3'], sample['coords'], sample['vis'], sample['shift']\n",
        "        image0 = image0.transpose((2, 0, 1))\n",
        "        image3 = image3.transpose((2, 0, 1))\n",
        "\n",
        "\n",
        "        if len(sample)==7:\n",
        "            image7 = sample['image7']\n",
        "            image7 = image7.transpose((2, 0, 1))\n",
        "            return {'id': sample_id,\n",
        "                    'image0': torch.from_numpy(image0),\n",
        "                    'image3': torch.from_numpy(image3),\n",
        "                    'image7': torch.from_numpy(image7),\n",
        "                    'coords': torch.from_numpy(coords),\n",
        "\t\t                'vis': torch.from_numpy(vis),\n",
        "                    'shift': torch.from_numpy(shift)}\n",
        "        else:\n",
        "            return {'id': sample_id,\n",
        "                    'image0': torch.from_numpy(image0),\n",
        "                    'image3': torch.from_numpy(image3),\n",
        "                    'coords': torch.from_numpy(coords),\n",
        "\t\t                'vis': torch.from_numpy(vis),\n",
        "                    'shift': torch.from_numpy(shift)}\n",
        "\n",
        "class AugmentData(object):\n",
        "    \"\"\"\n",
        "    Augment data with ColorJitter, Gaussian Noise and To Gray transformations.\n",
        "    \"\"\"\n",
        "    def __call__(self, sample):\n",
        "        sample_id, image0, image3, coords, vis, shift = sample['id'], sample['image0'], sample['image3'], sample['coords'], sample['vis'], sample['shift']\n",
        "        \n",
        "        trans = A.Compose(\n",
        "            [\n",
        "             A.augmentations.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.2),\n",
        "             A.augmentations.transforms.GaussNoise (var_limit=.05, mean=0, per_channel=True, always_apply=False, p=0.2),\n",
        "             A.augmentations.transforms.ToGray(p=0.1)\n",
        "             ],\n",
        "            additional_targets={'image3': 'image'}\n",
        "            )\n",
        "\n",
        "        image0 = image0.astype(np.float32)\n",
        "        image3 = image3.astype(np.float32)\n",
        "\n",
        "        transformed = trans(image=image0, image3=image3)\n",
        "        n_image0 = transformed['image']\n",
        "        n_image3 = transformed['image3']\n",
        "        \n",
        "        if len(sample)==7:\n",
        "            image7 = sample['image7']\n",
        "            return {'id': sample_id,\n",
        "                    'image0': n_image0,\n",
        "                    'image3': n_image3,\n",
        "                    'image7': image7,\n",
        "                    'coords': coords,\n",
        "\t\t            'vis': vis,\n",
        "                    'shift': shift}\n",
        "        else:\n",
        "            return {'id': sample_id,\n",
        "                    'image0': n_image0,\n",
        "                    'image3': n_image3,\n",
        "                    'coords': coords,\n",
        "\t\t            'vis': vis,\n",
        "                    'shift': shift}\n",
        "\n",
        "class ShiftData(object):\n",
        "    \"\"\"\n",
        "    Shifting data along random x, y axis.\n",
        "    \"\"\"\n",
        "    def __call__(self, sample):\n",
        "        sample_id, image0, image3, coords, vis, shift = sample['id'], sample['image0'], sample['image3'], sample['coords'], sample['vis'], sample['shift']\n",
        "\n",
        "        # randomly sample these values and pass to Affine trans (x in range (-5,5), y in range (-10,10)\n",
        "        random_x = np.random.randint(-5,5)\n",
        "        random_y = np.random.randint(-10,10)\n",
        "\n",
        "        coords[:, :, 0] =  coords[:, :, 0 ]+random_y\n",
        "        coords[:, :, 1] =  coords[:, :, 1 ]+random_x\n",
        "\n",
        "\n",
        "        trans = A.Compose(\n",
        "            [\n",
        "              A.augmentations.geometric.transforms.Affine(scale=1, translate_percent=None, translate_px={'x':random_x, 'y':random_y}, shear=None, fit_output=False, keep_ratio=False, always_apply=False, p=1)             \n",
        "            ],\n",
        "            additional_targets={'image3': 'image'}\n",
        "            )\n",
        "\n",
        "        image0 = image0.astype(np.float32)\n",
        "        image3 = image3.astype(np.float32)\n",
        "\n",
        "        transformed = trans(image=image0, image3=image3)\n",
        "        n_image0 = transformed['image']\n",
        "        n_image3 = transformed['image3']\n",
        "\n",
        "        shift = (random_x, random_y)\n",
        " \n",
        "        if len(sample)==7:\n",
        "            image7 = sample['image7']\n",
        "            return {'id': sample_id,\n",
        "                    'image0': n_image0,\n",
        "                    'image3': n_image3,\n",
        "                    'image7': image7,\n",
        "                    'coords': coords,\n",
        "\t\t            'vis': vis,\n",
        "                    'shift': np.array(shift)}\n",
        "        else:\n",
        "            return {'id': sample_id,\n",
        "                    'image0': n_image0,\n",
        "                    'image3': n_image3,\n",
        "                    'coords': coords,\n",
        "\t\t            'vis': vis,\n",
        "                    'shift': np.array(shift)}"
      ],
      "metadata": {
        "id": "F-WUtHY0f3JE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "5qULSTv6kXEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(device, model, dataloader):\n",
        "  model.eval()\n",
        "  \n",
        "  for i_batch, sample_batched in enumerate(dataloader):\n",
        "    batch_size = len(sample_batched['id'])\n",
        "\n",
        "    input1 = sample_batched['image0']\n",
        "    input2 = sample_batched['image3']\n",
        "    last_imgs = sample_batched['image7']\n",
        "    inputs = torch.cat((input1, input2), dim=1) \n",
        "    inputs = inputs.to(device).float() # torch.Size([B, 6, H, W])\n",
        "\n",
        "    with torch.no_grad():\n",
        "          # Get model outputs \n",
        "          outputs = model(inputs) # torch.Size([B, 3, H, W]) same as inputs shape\n",
        "          outputs = outputs['out'].to(device)\n",
        "          outputs_coords = outputs[:, :2, :, :]\n",
        "          outputs_vis = outputs[:, 2, :, :]\n",
        "          outputs_coords = torch.permute(outputs_coords, (0, 2, 3, 1))\n",
        "          outputs_vis = torch.where(outputs_vis > 0, 1.0, 0.0)\n",
        "          outputs_vis = outputs_vis.view(batch_size,1,-1)\n",
        "          outputs_coords = outputs_coords.view(batch_size,1,-1,2) # torch.Size([B, 1, 64*64, 2])\n",
        "\n",
        "          reconstruct_batch(outputs_coords.cpu().numpy(), outputs_vis.cpu().numpy(), input1, last_imgs)"
      ],
      "metadata": {
        "id": "cvcm1nCogW6Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_batch(coords, vis, inputs, gts):\n",
        "  for i in range(coords.shape[0]):\n",
        "    input=inputs[i].permute(1, 2, 0).cpu().numpy() # H, W, C\n",
        "    gt=gts[i].permute(1, 2, 0).cpu().numpy() # H, W, C\n",
        "\n",
        "    prediction = frame_reconstruction(input, coords[i], vis[i])\n",
        "\n",
        "    plt.imshow(input)\n",
        "    plt.title(\"First Frame\")\n",
        "    plt.show()\n",
        "    plt.imshow(gt)\n",
        "    plt.title(\"Last Frame\")\n",
        "    plt.show()\n",
        "    plt.imshow(prediction)\n",
        "    plt.title(\"Reconstructed Frame\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "K9yR39rShOJ1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frame_reconstruction(img0, coords, vis):\n",
        "  h = np.arange(0, 360) # Take linspace of H of the image\n",
        "  w = np.arange(0, 640) # Take linspace of W of the image\n",
        "  w, h = np.meshgrid(w, h)\n",
        "  original_x = h.flatten()\n",
        "  original_y = w.flatten()\n",
        "\n",
        "  vis = vis.squeeze() > 0\n",
        "  original_x = original_x[vis]\n",
        "  original_y = original_y[vis]\n",
        "\n",
        "  coords = coords.squeeze()\n",
        "  \n",
        "  coords_x = coords[:, 1]\n",
        "  coords_y = coords[:, 0]\n",
        "  coords_x = coords_x[vis]\n",
        "  coords_y = coords_y[vis]\n",
        "\n",
        "\n",
        "  coords_x = original_x+coords_x\n",
        "  coords_y = original_y+coords_y\n",
        "\n",
        "  \n",
        "  reconstructed_NN = interpolate(img0, original_x, original_y, coords_x, coords_y)\n",
        "\n",
        "  return reconstructed_NN"
      ],
      "metadata": {
        "id": "ddx7UDqBixsW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import NearestNDInterpolator\n",
        "def interpolate(img0, original_x, original_y, coords_x, coords_y):\n",
        "  z = img0\n",
        "  z_R = []\n",
        "  z_G = []\n",
        "  z_B = []\n",
        "\n",
        "  for point in list(zip(original_x, original_y)):\n",
        "    x, y = point\n",
        "    x = int(x)\n",
        "    y = int(y)\n",
        "    z_R.append(img0[x, y, 0]) #(360,640,3)\n",
        "    z_G.append(img0[x, y, 1])\n",
        "    z_B.append(img0[x, y, 2])\n",
        "  \n",
        "\n",
        "  X = np.arange(0, 360) # Take linspace of H of the image\n",
        "  Y = np.arange(0, 640) # Take linspace of W of the image\n",
        "  X, Y = np.meshgrid(X, Y)  # 2D grid for interpolation\n",
        "\n",
        "  points = list(zip(coords_x, coords_y))\n",
        "\n",
        "  interp_R_NN = NearestNDInterpolator(points, z_R) # predicted point coord -> R-intensity\n",
        "  interp_G_NN = NearestNDInterpolator(points, z_G) # predicted point coord -> G-intensity\n",
        "  interp_B_NN = NearestNDInterpolator(points, z_B) # predicted point coord -> B-intensity\n",
        "\n",
        "\n",
        "  Z_R_NN = interp_R_NN(X, Y) \n",
        "  Z_G_NN = interp_G_NN(X, Y)\n",
        "  Z_B_NN = interp_B_NN(X, Y)\n",
        "\n",
        "\n",
        "  concateneted_NN = np.stack([Z_R_NN, Z_G_NN, Z_B_NN])\n",
        "\n",
        "  img_NN = concateneted_NN\n",
        "\n",
        "  reconstruction_NN = img_NN.transpose(2, 1, 0)\n",
        "\n",
        "  return reconstruction_NN"
      ],
      "metadata": {
        "id": "lZU0AFHgiz5w"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Model"
      ],
      "metadata": {
        "id": "F0Fhkutfewpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose Dataset and Model Path\n",
        "DATASET_PATH = \"natural_data/\"\n",
        "MODEL_PATH = \"model2A.pt\"\n",
        "\n",
        "model = torch.load(MODEL_PATH)\n",
        "\n",
        "dataset = CustomDataset(log_file=DATASET_PATH+'/sample_ids.txt',\n",
        "                        root_dir=DATASET_PATH,\n",
        "                        check=True,\n",
        "                        transform=transforms.Compose([ToTensor()]))\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "device = 'cuda'\n",
        "run_model(device, model, dataloader)"
      ],
      "metadata": {
        "id": "a3WqRBekeu0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}